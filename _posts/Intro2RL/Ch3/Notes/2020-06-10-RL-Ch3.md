---
layout: post
title: "Chapter 3: Finite Markov Decision Processes"
notesFor: 'Intro2RL' 
noCode: true
categories: RL 
---

* The **finite Markov Decision Processes (MDP)** involves evaluative feedback, just like in bandits, but also an associative aspect choosing different actions in different situations.

* In MDPs, actions influence not just immediate rewards, but also subsequent situations or states, and through those future rewards.

$$S_0, A_0, R_1, S_1, A_1, R_2, S_2,A_2, R_3,...$$

* In MDP, The probability function p completely characterize the environment’s dynamics .which suggest that the probability of each possible value for $$S_it$$ and $$R_t$$ depends on the immediately preceding state and action, $$S_{t-1}$$ and $$A_{t-1}$$ which clearly follows the markov property,

    $$p(s', r|s,a) = Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a \}$$

* **State-transition probability** defines the probability of observing the state $$s$$ given the previous state and action. 

    $$p(s'|s,a) = Pr\{S_t = s'| S_{t-1} = s, A_{t-1} = a \} = \sum_{r \in \mathcal{R}}p(s',r | s,a)$$

    it proposes that whatever the details of the sensory, memory, control apparatus and whatever objective one are trying to achieve, *any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment*: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent’s goal (the rewards). This framework may not be sufficient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable.

* Informally, the goal of the agent is to *maximize the cumulative reward in the long run*. 

* The reward signal is your way of communicating to the agent what you want to achieve, not how you want it achieved.

* The **expected return** in which the “return” is defined as some specific function of the reward sequence. // add 3.7

* The subsequence which naturally breaks the agent-environment interaction leads is known as the episodes, each episode ends in a special state called the terminal state, followed by the reset.

## Policies and Value Functions

* A policy is a mapping from states to probabilities of selecting each possible action.
* The **value function** of a state $$s$$ under a policy $$\pi$$, denoted $$v_\pi(s)$$, is the expecte return when starting in $$s$$ and following $$\pi$$ thereafter.

    $$
    v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | S_{t}=s\right], \text { for all } s \in \delta
    $$

    where $$\mathbb{E}_{\pi}[\cdot]$$ denotes the expected value of a random variable given that the agent follows policy $$\pi,$$ and $$t$$ is any time step. Note that the value of the terminal state, if any, is always zero. We call the function $$v_{\pi}$$ the state-value function for policy $$\pi$$

    Similarly, we define the value of taking action $$a$$ in state $$s$$ under a policy $$\pi$$, denoted $$q_{\pi}(s, a),$$ as the expected return starting from $$s,$$ taking the action $$a,$$ and thereafter following policy $$\pi:$$

    $$
    q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s, A_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | S_{t}=s, A_{t}=a\right]
    $$
    We call $$q_{\pi}$$ the **action-value function** for policy $$\pi$$.


* Calculating the value functions $$v_\pi$$ and $$q_\pi$$:-

    * **Monte Carlo Methods** involves averaging over many random samples of actual returns for each state encountered and in theory, the average will converge to the state's value, $$v_\pi(s)$$ as the number of times that state encountered approaches infinity.

    * parameterized function: instead of storing separate averages for each state individually we can have a parameterized function of $$v_\pi$$ and $$q_\pi$$ with fewer parameters than states and adjust the parameters to better match the observed returns.

## Bellman Equation

* The Bellman equation averages over all the possibilities, weighting each by its probability of occurring. 

* Bellman equation for $$v_\pi$$ :-

$$
v_\pi(s) = \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right] = \sum_{a} \pi(a|s) \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_\pi (s')]}
$$

* Bellman equation for $$q_\pi$$ :-
$$
v_\pi(s, a) = \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s, A_{t} = a\right] =  \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma \sum_{a'} \pi(a'|s') q_\pi (s', a')]}
$$

<div style="margin: 0 auto; text-align: center">
    <img src="{{baseUrl}}/assets/Imgs/RL/Ch3/BackUpDiag.png"  width="300px" />
</div>

## Optimal Policies and Optimal Value Functions

* A policy $$\pi$$ is defined to be better than or equal to policy $$\pi'$$ if its expected return is greater than or equal to that of $$\pi'$$ for all states 

* there is at least one policy that is better than or equal to all other policies. this is an **optimal policy**.

* The Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state.


* Bellman optimality equation for $$v_\pi$$ :-

$$
v_*(s) = \max_{a} \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma v_\pi (s')]}
$$

* Bellman optimality equation for $$q_\pi$$ :-

$$
q_*(s, a) = \sum_{s', r}{p(s', r | s, a)}{[ r + \gamma \max_{a'} q_* (s', a')]}
$$

## Optimality and Approximation

even if we have a complete and accurate model of the environment's dynamics, it is usually not possible to simply compute an optimal policy by solving the Bellman optimality equation.