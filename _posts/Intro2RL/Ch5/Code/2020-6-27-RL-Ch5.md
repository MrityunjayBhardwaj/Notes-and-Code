---
layout: post
title: "Chapter 5: Monte Carlo Methods"
codeFor: 'Intro2RL' 
categories: RL 
---


<script src="{{site.sub_url}}/assets/viz/dependency/tensorflowJS/tf.min.js"></script>
<script src="{{site.sub_url}}/assets/viz/dependency/plotlyJS/plotly-latest.min.js"></script>
<script src="{{site.sub_url}}/assets/viz/dependency/utils.js"></script>
<link rel="stylesheet" type="text/css" href="{{site.sub_url}}/assets/viz/code/ch5/style.css"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500|Material+Icons" rel="stylesheet" type="text/css">
<script src="{{site.sub_url}}/assets/viz/dependency/d3.js"></script>
<script src="{{site.sub_url}}/assets/viz/dependency/lodash.js"></script>

<div id="mainContainer">

<div id="properties">

  <div id="controls">
    <button id="restart">&#x21ba;</button>
    <button id="pausePlay" class="material-icons">pause</button>
    <button id="nextStep" class="material-icons md-48">skip_next</button>
  </div>

  <div>
    <h4 id="epochName">Epoch</h4>
    <input type="number" id="epoch" min="0" value="5"/>
  </div>

  <div>
    <h4 id="epsilonName">Epsilon</h4>
    <input type="number" id="epsilon" min="0" />
  </div>

  <div>
    <h4 id="epsilonName">No-animation (faster)</h4>
    <input type="checkbox" id="noAnim" min="0" />
  </div>
</div>
<div id="BJ">
  <div id="left-panel">
    <div id="score">
      <div id="dealerScore"> Dealer Score: 5</div>
      <div id="playerScore">Player Score: 21</div>
    </div>
    <div id="episodeLog" style="width: 200px; padding: 0px">
      <h4 style="text-decoration: underline; text-decoration-skip-ink: none; text-align: left;">Episode #<span id="episodeNumber">3</span> Logs</h4>
      <ul id="episodeLogList">
      </ul>
    </div>
  </div>
  <div id="vizContainer">
    <div id="blackjackArea">

    </div>
  <div id="outputs">
      <div id="playerAction">
        <div class="material-icons" >
        east
        </div>
        <div id="hitAction">Hit</div>
        <div id="standAction">Stand</div>
      </div>
      <div id="gameStatus"></div>
    </div>
  </div>

  
</div>

<hr>

  <div id="valueFnContainer">
    <h2 style="text-decoration-skip-ink: none">Plots</h2>
    <p>Lorem ipsum dolor sit amet consectetur adipisicing elit. Adipisci cupiditate fuga nostrum facere nisi, eius perferendis culpa ullam ducimus nemo sit sint at soluta eos natus sapiente maiores? Tempora, beatae!</p>
    <div id="valueFnViz" >
      <div id="valueFnNoAceViz" style="width: 500px; height: 500px; "></div>
      <div id="valueFnAceViz" style="width: 500px; height: 500px; "></div>
    </div>
  </div>

<hr>  
<div id="codeContainer" markdown="1" >
### mcAlgo.js

<div id="code"  markdown="1">


```javascript
class MC {

  constructor(env, params, callback) {
      this._model = {
        stateValue : [],
        stateActionValue: [], 
        weightsCumSum : 0,
        policy: new Array(100),
        env: env,
        params : {
          epsilon: params.epsilon || 0.5,
          discountFactor: params.discountFactor || 1.0,
        }
    }
    this._nA = this._model.env.nA;

    // initializinng Q values
    this._model.qValue = new HashMap();

    // initializing cummulative weights
    this._model.weightsCumSum =  new HashMap();

    // initializing target Policy Fn
    this._model.policyFn = (state) =>{
      const A = _.fill(new Array(this._nA), 0);
      if (!this._model.qValue.has(state)){
        A[0] = 1;
        return defaultActionValue;
      }
      const greedyAction = argMax(this._model.qValue.get(state))
      A[greedyAction] = 1;
      return A;
    };



    this.callback = callback || (()=>{});
  }

  /**
   * @summary returns the qValue of our model
   * @returns {HashMap}
   */
  getQValue(){
    return this._model.qValue;
  }

  /**
   * @summary returns the target Policy function using our qValue estimates
   * @returns {function}
   */
  getPolicy(){ // targetPolicy
    return this._model.policyFn;
  }

  /**
   * 
   * @param {number} maxEpisode how many episode should we allow our agent to run
   * @param {function} behaviorPolicy our proposed policy from which we will choose our action
   * @param {number} discountFactor weights associated with each of our returns
   * 
   * @returns {object} the model itself ( for function chaining)
   */
  async train (maxEpisode, behaviorPolicy){

      const discountFactor = this._model.params.discountFactor;

      let cEpisode = 0;

      // NOTE: using interval for visualization purpose
      while(true){

        // deal();

        if (cEpisode % maxEpisode/10 === 0)
          console.log(`${cEpisode}) `);
    
        // * Generating the episodes
        const nSteps = 100;

        //storing our state, action, reward pairs for this episode
        const episodePairs = [];
        let forViz = null;
        let state = this._model.env.reset();

        // actual episode loop
        for(let t=0; t< nSteps; t+=1){

          // choose a random action
          const action = Math.floor(Math.random()*this._nA);

          // store all the pairs 
          const { observation: nextState, reward, isDone, cardUsed } = this._model.env.step(action);

          episodePairs.push({state, action, reward});

          // stop this episode if it reaches the terminal state
          if (isDone){

            forViz = cardUsed;
            break;
          }

          state = nextState;

        }

        // * backtrack the episode for estimating our Q values

        // initializing
        let returnsSum = 0;
        let returnsWeights = 1;

        // actual backtracking loop
        for(let i=episodePairs.length-1;i >= 0; i -=1){

          // fetching our state action and reward pairs from the current time step
          const {state, action, reward} = episodePairs[i];

          // caculating our sum of returns
          returnsSum = discountFactor * returnsSum + reward;

          // calculating our cummulative sum of weights ( importance ratio)
          const c = this._model.weightsCumSum;
          let cWeightsCumSum = (c.has(state))? c.get(state) : _.fill(Array(this._nA), 0);
          cWeightsCumSum[action] += returnsWeights;
          c.set(state, cWeightsCumSum)

          // calculating and storing our Q values
          const q = this._model.qValue;
          let newActionVal = (q.has(state))? q.get(state) : _.fill(Array(this._nA), 0);
          newActionVal[action] += (returnsWeights / c.get(state)[action]) * (returnsSum - newActionVal[action]);
          q.set(state, newActionVal)

          // if its not an optimal policy then simply break the loop
          if (action != argMax(this._model.policyFn(state)))
            break;

          // updating our  importance sampling ratio
          returnsWeights = returnsWeights * 1/behaviorPolicy(state)[action];
        }
    
        if(cEpisode % 10 === 0)
          console.log(`${cEpisode}) returnSum: ${returnsWeights}`);

          console.log('before callback');

        const stopTheLoop = await this.callback(episodePairs, forViz) || 0;

        console.log('executed the callback function', stopTheLoop);

        if(cEpisode >= maxEpisode || stopTheLoop){
          console.log('exiting');  
          break;
        }

        cEpisode +=1;


      }

    return this;
  }

}
```

</div>

  </div>
</div>
<script src="{{site.sub_url}}/assets/viz/dependency/lodash.js"></script>
<script src="{{site.sub_url}}/assets/viz/code/ch5/blackJackGym.js"></script>
<!-- <script src="Source/mcWithImportanceSampling.js"></script> -->
<script src="{{site.sub_url}}/assets/viz/code/ch5/Source/monteCarloAlgo.js"></script>
<script src="{{site.sub_url}}/assets/viz/code/ch5/Source/mcViz.js"></script>


