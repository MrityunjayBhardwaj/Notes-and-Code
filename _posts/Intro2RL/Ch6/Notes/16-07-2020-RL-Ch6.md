---
layout: post
title: "Chapter 6: Monte Carlo Methods"
notesFor: 'Intro2RL'
categories: RL 
---

> If one had to identify 1 idea as central and novel to reinforcement learning it would undoubtedly by temporal-difference learning - Authors

<!-- * Like MC methods, TD methods can learn directly from raw experience without a model of the environment's dynamics.
* Like DP, TD also do bootstrapping to update there estimates. -->
* TD methods combine the sampling of Monte Carlo with the bootstrapping of DP.

* for control problem, DP, TD, and Monte Carlo methods all use some variation of Generalized policy iteration (GPI). *its the prediction problem they tend to differ*.

## TD prediction

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.sub_url}}/assets/Imgs/RL/Ch6/TD-eqn.png"  width="50%" />
</div>

* the simplest TD method makes the update immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. using $R_{t+1} + \gamma V(S_{t+1})$ as a target instead of just $R_{t+1}$ (like in MC methods). this TD method is known as TD(0)

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.sub_url}}/assets/Imgs/RL/Ch6/TD(0)-algo.png"  width="80%" />
</div>

* The TD target is an estimate because it samples the expected values $\mathbb{E}_\pi[ R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t =s]$ and it uses the current estimate $V$ instead of the true $v_\pi$.

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.sub_url}}/assets/Imgs/RL/Ch6/TD-backup-diag.png"
      width="80px" transform="rotate(90deg)"/>
</div>

* we refer to TD and MC updates as samples updates as well because they are based on single sample sucessor rather than on a complete distribution of all possible sucessors (like in DP)

## Advantages of TD Prediction Methods

* They bootstrap
* they do not require a model 
* TD methods need to wait for only one time step which means they can be implemented in an online fashion which is highly beinficial in a really long episode setting or in continuing tasks where MC methods might not even work( because there is no end).
* just like DP and MC, TD also has some converge guarantees but currently we don't exactly know which one converges faster.

## Optimality of TD(0)

* the MC methods and TD methods converges deterministically under the same conditions, but to a *different answer*.
* it turns out that in Bach updating setting, the Batch MC methods always find the estimates that minize the MSE on the training set whereas *batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the markov process.*

## Sarsa: On-policy TD Control

* Used for calculating the state-action values instead of state values.

* The update expression for prediction task in sarsa looks like this:-
<div style="margin: 0 auto; text-align: center">
    <img src="{{site.sub_url}}/assets/Imgs/RL/Ch6/sarsa-eqn.png"
      width="70%" />
</div>

* for full control task we use the above expression and combine it with any policy impovement techniques that we have seen uptill now (like $\epsilon$-greedy, UCB etc)
  

## Q-learning: Off-policy TD Control

* because its off-policy we use 2 different policy for action selection and for calculating our estimates.
  * more specifically, it uses exploratory policy for action selection and greedy policy for estimating the q-values:-
<div style="margin: 0 auto; text-align: center">
    <img src="{{site.sub_url}}/assets/Imgs/RL/Ch6/Q-learning-eqn.png"
      width="70%" />
</div>

* it will converge to optimal policy as long as we follow a soft-policy for action selection
* Maximization Bias: because it takes the max over the q-values of the next state-actions pairs, it selects overestimated values which affects the agent's performance 

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.sub_url}}/assets/Imgs/RL/Ch6/Q-learning-algo.png"
      width="70%" />
</div>
## Expected Sarsa
* here, instead of maximizing over the next state-action pairs,we take the expectation instead
* its computationally much more expensive

## Double Q-learning

* in Double Q-learning we use 2 different q-value tables instead of one and randomely select one of them for updating
* for action selection, we can also take the combination of both the q-values (e.g. $\frac{Q_1 + Q_2}{2}$ maybe?) instead of using only one.
* it doesn't have maximization bias

<div style="margin: 0 auto; text-align: center">
    <img src="{{site.sub_url}}/assets/Imgs/RL/Ch6/Double-Q-learning-algo.png"
      width="80%" />
</div>

* we can also use the similar ideas to extend Sarsa and Expected sarsa respectively

> its important to note that all these methods are just some form of TD learning but in practice we treat them differently.

## Afterstate

* In tic-tac-toe example (ch1) we evaluate the board position *after* the agent has made its move. that's why its called **afterstates**  and value function over them are known as afterstate value function
* it is useful when we have partial knowledge of the environment's dynamics. for e.g. we only knows about the initial part of the env.
* sometimes prevents redundency among different states.
* can also seamlessly be integrated into our GPI framework.